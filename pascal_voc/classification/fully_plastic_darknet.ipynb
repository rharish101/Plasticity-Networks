{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from load_pvoc_data import load_data, TRAIN_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "VALIDATION_SPLIT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(img, lbl):\n",
    "    crop_img = tf.image.central_crop(img, 1)\n",
    "    resized = tf.image.resize_images(img, (256, 256))\n",
    "    norm_img = tf.image.per_image_standardization(resized)\n",
    "    \n",
    "    one_hot = tf.one_hot(lbl, 20)\n",
    "    summed = tf.reduce_sum(one_hot, axis=-2)\n",
    "    multi_hot = tf.where(\n",
    "        tf.equal(summed, 0), tf.zeros_like(summed, dtype=tf.float32), tf.ones_like(summed, dtype=tf.float32)\n",
    "    )\n",
    "    return norm_img, multi_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    train_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda:load_data(\"train\"),\n",
    "        (tf.uint8, tf.int32),\n",
    "        (tf.TensorShape([None, None, 3]), tf.TensorShape([None]))\n",
    "    ).map(preprocessing).shuffle(10000)\n",
    "    train_dataset = train_dataset.apply(tf.contrib.data.assert_element_shape((\n",
    "        [256, 256, 3],\n",
    "        [20]\n",
    "    )))\n",
    "    \n",
    "    val_length = int(VALIDATION_SPLIT * TRAIN_LENGTH)\n",
    "    val_dataset = train_dataset.take(val_length).apply(\n",
    "        tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE))\n",
    "    train_dataset = train_dataset.skip(val_length).apply(\n",
    "        tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE)).repeat()\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_input_fn():\n",
    "    test_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda:load_data(\"test\"),\n",
    "        (tf.uint8, tf.int32),\n",
    "        (tf.TensorShape([None, None, 3]), tf.TensorShape([None]))\n",
    "    )\n",
    "    test_dataset = test_dataset.map(preprocessing).apply(tf.contrib.data.assert_element_shape((\n",
    "        [256, 256, 3],\n",
    "        [20]\n",
    "    )))\n",
    "    return test_dataset.apply(tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(inputs, eta, scope_name, filters=32, kernel_size=3, strides=1, plastic=True):\n",
    "    with tf.variable_scope(scope_name):\n",
    "        w = tf.get_variable('conv_w', (kernel_size, kernel_size, int(inputs.shape[-1]), filters))\n",
    "        b = tf.get_variable('conv_b', (filters,))\n",
    "        if plastic:\n",
    "            alpha = tf.get_variable('conv_alpha', (kernel_size, kernel_size, int(inputs.shape[-1]), filters))\n",
    "            hebb = tf.get_variable('conv_hebb', (kernel_size, kernel_size, int(inputs.shape[-1]), filters),\n",
    "                                   trainable=False, initializer=tf.zeros_initializer)\n",
    "            w = w + tf.multiply(alpha, hebb)\n",
    "    \n",
    "    x = tf.nn.conv2d(input=inputs, filter=w, strides=[1, strides, strides, 1], padding='SAME') + b\n",
    "    \n",
    "    if plastic:\n",
    "        # y is to be the output reshaped so as to be used as a kernel for convolution on input to get Hebbian update\n",
    "        y = tf.image.resize_images(x, [int(inputs.shape[1])] * 2)\n",
    "        y = tf.transpose(y, [1, 2, 0, 3])\n",
    "\n",
    "        # in_mod is the input padded a/c to prev. convolution\n",
    "        in_mod = tf.pad(inputs, [\n",
    "            [0, 0],\n",
    "            *([[int(np.floor((kernel_size - 1) / 2)), int(np.ceil((kernel_size - 1) / 2))]] * 2),\n",
    "            [0, 0]\n",
    "        ])\n",
    "        # in_mod is now modded so as to preserve channels and sum over mini-batch samples for Hebbian update convolution\n",
    "        in_mod = tf.transpose(in_mod, [3, 1, 2, 0])\n",
    "\n",
    "        hebb_update = tf.nn.conv2d(input=in_mod, filter=y, strides=([1] * 4), padding='VALID')\n",
    "        hebb = eta * tf.transpose(hebb_update, [1, 2, 0, 3]) + (1 - eta) * hebb\n",
    "    \n",
    "    return tf.nn.relu(tf.layers.batch_normalization(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(inputs, filters, scope_name, eta):\n",
    "    with tf.variable_scope(scope_name):\n",
    "        x = conv_layer(inputs, eta, scope_name='blk_layer_1', filters=filters, kernel_size=1)\n",
    "        x = conv_layer(inputs, eta, scope_name='blk_layer_2', filters=(filters * 2))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def darknet_block(inputs, filters, repetitions, scope_name, eta):\n",
    "    with tf.variable_scope(scope_name):\n",
    "        x = conv_layer(inputs, eta, scope_name='blk_layer_0', filters=filters, strides=2)\n",
    "        for i in range(repetitions):\n",
    "            x = residual_block(x, filters / 2, scope_name='blk_rep_' + str(i), eta=eta)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def darknet_model(features, labels, mode):  \n",
    "    features = tf.cast(features, dtype=tf.float32)\n",
    "    \n",
    "    eta = tf.get_variable('eta', (1,), initializer=tf.truncated_normal_initializer)\n",
    "    \n",
    "    x = conv_layer(features, eta, scope_name='first', filters=32)\n",
    "    x = darknet_block(x, filters=64, repetitions=1, scope_name='dark_blk_0', eta=eta)\n",
    "    x = darknet_block(x, filters=128, repetitions=2, scope_name='dark_blk_1', eta=eta)\n",
    "    x = darknet_block(x, filters=256, repetitions=8, scope_name='dark_blk_2', eta=eta)\n",
    "    x = darknet_block(x, filters=512, repetitions=8, scope_name='dark_blk_3', eta=eta)\n",
    "    x = darknet_block(x, filters=1024, repetitions=4, scope_name='dark_blk_4', eta=eta)\n",
    "    \n",
    "    x = tf.layers.average_pooling2d(x, pool_size=8, strides=8)\n",
    "    x = tf.layers.flatten(x)\n",
    "    x = tf.layers.dense(inputs=x, units=20, kernel_initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    classes = tf.where(tf.sigmoid(x) >= 0.5, tf.ones_like(x, dtype=tf.float32), tf.zeros_like(x, dtype=tf.float32))\n",
    "    correct_prediction = tf.equal(classes, labels)\n",
    "    acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    acc = tf.identity(acc, name='accuracy_tensor')\n",
    "    \n",
    "    predictions = {'classes': classes, 'accuracy': acc}\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    \n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=x)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    tf.summary.scalar('accuracy', acc)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.contrib.estimator.TowerOptimizer(tf.train.AdamOptimizer(1e-4))\n",
    "        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    eval_metric_ops = {'accuracy': tf.metrics.accuracy(labels=labels, predictions=classes)}\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss,\n",
    "                                      eval_metric_ops=eval_metric_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors_to_log = {'accuracy': 'accuracy_tensor'}\n",
    "logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.estimator.Estimator(\n",
    "    model_fn=tf.contrib.estimator.replicate_model_fn(darknet_model),\n",
    "    model_dir='/tmp/tmpfull', config=tf.estimator.RunConfig(\n",
    "        save_checkpoints_steps=150, save_summary_steps=10, log_step_count_steps=10\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_hook = tf.contrib.learn.monitors.replace_monitors_with_hooks(\n",
    "    [tf.contrib.learn.monitors.ValidationMonitor(\n",
    "        input_fn=lambda:train_input_fn()[1], every_n_steps=100, early_stopping_rounds=10\n",
    "    )],\n",
    "    model\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_steps = int(((1 - VALIDATION_SPLIT) * TRAIN_LENGTH / BATCH_SIZE) * EPOCHS)\n",
    "model.train(input_fn=lambda:train_input_fn()[0], hooks=[logging_hook, validation_hook],\n",
    "            max_steps=max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.evaluate(input_fn=test_input_fn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
