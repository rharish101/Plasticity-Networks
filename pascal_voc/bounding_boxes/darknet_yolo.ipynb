{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from load_pvoc_data import load_data, TRAIN_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 4\n",
    "VALIDATION_SPLIT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(img, lbl):\n",
    "    #one_hot = tf.one_hot(lbl, 20)\n",
    "    #summed = tf.reduce_sum(one_hot, axis=-2)\n",
    "    #multi_hot = tf.where(\n",
    "    #    tf.equal(summed, 0), tf.zeros_like(summed, dtype=tf.float32), tf.ones_like(summed, dtype=tf.float32)\n",
    "    #)\n",
    "    return img, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    train_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda:load_data(\"train\"),\n",
    "        (tf.uint8, tf.int32),\n",
    "        (tf.TensorShape([None, None, 3]), tf.TensorShape([None]))\n",
    "    )\n",
    "    train_dataset = train_dataset.shuffle(10000)\n",
    "    \n",
    "    val_length = int(VALIDATION_SPLIT * TRAIN_LENGTH * 8)\n",
    "    val_dataset = train_dataset.take(val_length).apply(\n",
    "        tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE))\n",
    "    train_dataset = train_dataset.skip(val_length).apply(\n",
    "        tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE)).repeat()\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_input_fn():\n",
    "    test_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda:load_data(\"test\"),\n",
    "        (tf.uint8, tf.int32),\n",
    "        (tf.TensorShape([None, None, 3]), tf.TensorShape([None]))\n",
    "    )\n",
    "    return test_dataset.apply(tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(inputs, filters=32, kernel_size=3, strides=1, activation=tf.nn.leaky_relu, batch_normalize=True\n",
    "               trainable=True):\n",
    "    x = tf.layers.conv2d(inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides, padding='same',\n",
    "                         trainable=trainable)\n",
    "    if batch_normalize:\n",
    "        x = tf.layers.batch_normalization(x, trainable=trainable)\n",
    "    if activation is not None:\n",
    "        x = activation(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(inputs, filters, trainable=False):\n",
    "    x = conv_layer(inputs=inputs, filters=filters, kernel_size=1, trainable=trainable)\n",
    "    x = conv_layer(inputs=inputs, filters=(filters * 2), trainable=trainable)\n",
    "    return x + inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def darknet_block(inputs, filters, repetitions, trainable=False):\n",
    "    x = conv_layer(inputs=inputs, filters=filters, strides=2, trainable=trainable)\n",
    "    for i in range(repetitions):\n",
    "        x = residual_block(x, filters / 2, trainable=trainable)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_layer(inputs, anchors):\n",
    "    indices_w = tf.range(int(inputs.shape[2]))\n",
    "    indices_h = tf.range(int(inputs.shape[1]))\n",
    "    x_indices, y_indices = tf.meshgrid(indices_w, indices_h)\n",
    "    \n",
    "    for i, anchor in enumerate(anchors):\n",
    "        inputs[:,:,:,25 * i + 0] = (tf.sigmoid(inputs[:,:,:,25 * i + 0]) + x_indices) / int(inputs.shape[2])    # bx\n",
    "        inputs[:,:,:,25 * i + 1] = (tf.sigmoid(inputs[:,:,:,25 * i + 0]) + y_indices) / int(inputs.shape[1])    # by\n",
    "        inputs[:,:,:,25 * i + 2] = (tf.exp(inputs[:,:,:,25 * i + 2]) * anchor[0]) / int(inputs.shape[2])    # bw\n",
    "        inputs[:,:,:,25 * i + 3] = (tf.exp(inputs[:,:,:,25 * i + 3]) * anchor[1]) / int(inputs.shape[1])    # bh\n",
    "        inputs[:,:,:,25 * i + 4] = tf.sigmoid(inputs[:,:,:,25 * i + 4])\n",
    "        \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppr(*args):\n",
    "    for i, arg in enumerate(args):\n",
    "        for j in range(3):\n",
    "            arg[:,:,:,25 * j + 0] = arg[:,:,:,25 * j + 1] - arg[:,:,:,25 * j + 3] / 2    # y_min\n",
    "            arg[:,:,:,25 * j + 1] = arg[:,:,:,25 * j + 0] - arg[:,:,:,25 * j + 2] / 2    # x_min\n",
    "            arg[:,:,:,25 * j + 2] = arg[:,:,:,25 * j + 1] + arg[:,:,:,25 * j + 3] / 2    # y_max\n",
    "            arg[:,:,:,25 * j + 3] = arg[:,:,:,26 * j + 0] + arg[:,:,:,25 * j + 2] / 2    # x_max\n",
    "        flattened = tf.reshape(\n",
    "            arg,\n",
    "            (-1, int(arg.shape[1] * arg.shape[2]), int(arg.shape[3]))\n",
    "        )\n",
    "        args[i] = tf.concat([flattened[:,:,:25], flattened[:,:,25:50],\n",
    "                             flattened[:,:,50:]], axis=1)\n",
    "    args = tf.concat(args, axis=1)\n",
    "    return tf.map_fn(\n",
    "        lambda boxes: tf.gather(boxes, tf.image.non_max_suppression(\n",
    "            boxes[:,:4],\n",
    "            boxes[:,4],\n",
    "            6,\n",
    "            score_threshold=0.5\n",
    "        )),\n",
    "        args,\n",
    "        infer_shape=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def darknet_model(features, labels, mode):  \n",
    "    features = tf.cast(features, dtype=tf.float32)\n",
    "    normalized = tf.map_fn(tf.image.per_image_standardization, features,\n",
    "                           infer_shape=False)\n",
    "    \n",
    "    # Feature extractor: Darknet53\n",
    "    x = conv_layer(inputs=normalized, filters=32, trainable=False)\n",
    "    x = darknet_block(x, 64, 1)\n",
    "    x = darknet_block(x, 128, 2)\n",
    "    l_36 = darknet_block(x, 256, 8)\n",
    "    l_61 = darknet_block(l_36, 512, 8)\n",
    "    x = darknet_block(l_61, 1024, 4, trainable=True)\n",
    "    \n",
    "    # YOLO model\n",
    "    x = conv_layer(x, filters=512, kernel_size=1)\n",
    "    x = conv_layer(x, filters=1024)\n",
    "    x = conv_layer(x, filters=512, kernel_size=1)\n",
    "    x = conv_layer(x, filters=1024)\n",
    "    l_79 = conv_layer(x, filters=512, kernel_size=1)\n",
    "    \n",
    "    x = conv_layer(l_79, filters=1024)\n",
    "    x = conv_layer(x, filters=75, kernel_size=1, activation=None, batch_normalize=False)\n",
    "    o_1 = yolo_layer(x, anchors=[(116, 90), (156, 198), (373, 326)])\n",
    "    \n",
    "    x = conv_layer(l_79, filters=256, kernel_size=1)\n",
    "    x = tf.image.resize_images(x, (int(x.shape[1]) * 2, int(x.shape[2]) * 2))\n",
    "    x = tf.concat([x, l_61], axis=-1)\n",
    "    x = conv_layer(x, filters=256, kernel_size=1)\n",
    "    x = conv_layer(x, filters=512)\n",
    "    x = conv_layer(x, filters=256, kernel_size=1)\n",
    "    x = conv_layer(x, filters=512)\n",
    "    l_91 = conv_layer(x, filters=256, kernel_size=1)\n",
    "    \n",
    "    x = conv_layer(x, filters=512)\n",
    "    x = conv_layer(x, filters=75, kernel_size=1, activation=None, batch_normalize=False)\n",
    "    o_2 = yolo_layer(x, anchors=[(30, 61), (62, 45), (59, 119)])\n",
    "    \n",
    "    x = conv_layer(l_91, filters=128, kernel_size=1)\n",
    "    x = tf.image.resize_images(x, (int(x.shape[1]) * 2, int(x.shape[2]) * 2))\n",
    "    x = tf.concat([x, l_36], axis=-1)\n",
    "    x = conv_layer(x, filters=128, kernel_size=1)\n",
    "    x = conv_layer(x, filters=256)\n",
    "    x = conv_layer(x, filters=128, kernel_size=1)\n",
    "    x = conv_layer(x, filters=256)\n",
    "    x = conv_layer(x, filters=128, kernel_size=1)\n",
    "    x = conv_layer(x, filters=256)\n",
    "    x = conv_layer(x, filters=75, kernel_size=1, activation=None, batch_normalize=False)\n",
    "    o_3 = yolo_layer(x, anchors=[(10, 13), (16, 30), (33, 23)])\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # Non-maximum suppression to remove overlapping boxes\n",
    "        output = non_max_suppr([o_1, o_2, o_3])\n",
    "        predictions = {\n",
    "            'images': tf.image.draw_bounding_boxes(features, output[:,:,:4]),\n",
    "            'labels': tf.argmax(output[:,:,5:], axis=-1)\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    \n",
    "    # TODO: Loss\n",
    "    \"\"\"\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=x)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    \"\"\"\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.contrib.estimator.TowerOptimizer(tf.train.AdamOptimizer(1e-4))\n",
    "        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_warm = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_start = tf.estimator.WarmStartSettings(ckpt_to_initialize_from='/tmp/tmpdark/', vars_to_warm_start=vars_warm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.estimator.Estimator(\n",
    "    model_fn=tf.contrib.estimator.replicate_model_fn(darknet_model), model_dir='/tmp/tmpdarkyolo',\n",
    "    warm_start_from=warm_start, config=tf.estimator.RunConfig(\n",
    "        save_checkpoints_steps=150, save_summary_steps=10, log_step_count_steps=10\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_hook = tf.contrib.learn.monitors.replace_monitors_with_hooks(\n",
    "    [tf.contrib.learn.monitors.ValidationMonitor(\n",
    "        input_fn=lambda:train_input_fn()[1], every_n_steps=100, early_stopping_rounds=10\n",
    "    )],\n",
    "    model\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_steps = int(((1 - VALIDATION_SPLIT) * TRAIN_LENGTH * 8 / BATCH_SIZE) * EPOCHS)\n",
    "model.train(input_fn=lambda:train_input_fn()[0], hooks=[validation_hook],\n",
    "            max_steps=max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.evaluate(input_fn=test_input_fn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
