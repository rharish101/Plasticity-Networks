{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from load_pvoc_data import load_data, TRAIN_LENGTH\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 4\n",
    "VALIDATION_SPLIT = 0.3\n",
    "L_COORD = 5\n",
    "L_NOOBJ = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    train_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda:load_data(\"train\"),\n",
    "        (tf.uint8, tf.int32),\n",
    "        (tf.TensorShape([None, None, 3]), tf.TensorShape([None]))\n",
    "    )\n",
    "    train_dataset = train_dataset.shuffle(10000)\n",
    "    \n",
    "    val_length = int(VALIDATION_SPLIT * TRAIN_LENGTH * 8)\n",
    "    val_dataset = train_dataset.take(val_length).apply(\n",
    "        tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE))\n",
    "    train_dataset = train_dataset.skip(val_length).apply(\n",
    "        tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE)).repeat()\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_input_fn():\n",
    "    test_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda:load_data(\"test\"),\n",
    "        (tf.uint8, tf.int32),\n",
    "        (tf.TensorShape([None, None, 3]), tf.TensorShape([None]))\n",
    "    )\n",
    "    return test_dataset.apply(tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(inputs, filters=32, kernel_size=3, strides=1, activation=tf.nn.leaky_relu, batch_normalize=True\n",
    "               trainable=True):\n",
    "    x = tf.layers.conv2d(inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides, padding='same',\n",
    "                         trainable=trainable)\n",
    "    if batch_normalize:\n",
    "        x = tf.layers.batch_normalization(x, trainable=trainable)\n",
    "    if activation is not None:\n",
    "        x = activation(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(inputs, filters, trainable=False):\n",
    "    x = conv_layer(inputs=inputs, filters=filters, kernel_size=1, trainable=trainable)\n",
    "    x = conv_layer(inputs=inputs, filters=(filters * 2), trainable=trainable)\n",
    "    return x + inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def darknet_block(inputs, filters, repetitions, trainable=False):\n",
    "    x = conv_layer(inputs=inputs, filters=filters, strides=2, trainable=trainable)\n",
    "    for i in range(repetitions):\n",
    "        x = residual_block(x, filters / 2, trainable=trainable)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_layer(inputs, anchors):\n",
    "    indices_w = tf.range(int(inputs.shape[2]))\n",
    "    indices_h = tf.range(int(inputs.shape[1]))\n",
    "    x_indices, y_indices = tf.meshgrid(indices_w, indices_h)\n",
    "    \n",
    "    for i, anchor in enumerate(anchors):\n",
    "        inputs[:,:,:,25 * i + 0] = (tf.sigmoid(inputs[:,:,:,25 * i + 0]) + x_indices) / int(inputs.shape[2])    # bx\n",
    "        inputs[:,:,:,25 * i + 1] = (tf.sigmoid(inputs[:,:,:,25 * i + 0]) + y_indices) / int(inputs.shape[1])    # by\n",
    "        inputs[:,:,:,25 * i + 2] = (tf.exp(inputs[:,:,:,25 * i + 2]) * anchor[0]) / int(inputs.shape[2])    # bw\n",
    "        inputs[:,:,:,25 * i + 3] = (tf.exp(inputs[:,:,:,25 * i + 3]) * anchor[1]) / int(inputs.shape[1])    # bh\n",
    "        inputs[:,:,:,25 * i + 4] = tf.sigmoid(inputs[:,:,:,25 * i + 4])\n",
    "        \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppr(*args):\n",
    "    for i, arg in enumerate(args):\n",
    "        for j in range(arg.shape[-1] / 25):\n",
    "            arg[:,:,:,25 * j + 0] = arg[:,:,:,25 * j + 1] - arg[:,:,:,25 * j + 3] / 2    # y_min\n",
    "            arg[:,:,:,25 * j + 1] = arg[:,:,:,25 * j + 0] - arg[:,:,:,25 * j + 2] / 2    # x_min\n",
    "            arg[:,:,:,25 * j + 2] = arg[:,:,:,25 * j + 1] + arg[:,:,:,25 * j + 3] / 2    # y_max\n",
    "            arg[:,:,:,25 * j + 3] = arg[:,:,:,25 * j + 0] + arg[:,:,:,25 * j + 2] / 2    # x_max\n",
    "        flattened = tf.reshape(\n",
    "            arg,\n",
    "            (-1, int(arg.shape[1] * arg.shape[2]), int(arg.shape[3]))\n",
    "        )\n",
    "        to_concat = []\n",
    "        for j in range(arg.shape[-1] / 25):\n",
    "            to_concat.append(flattened[:,:,(25 * j):(25 * (j + 1))])\n",
    "        args[i] = tf.concat(to_concat, axis=1)\n",
    "    args = tf.concat(args, axis=1)\n",
    "    return tf.map_fn(\n",
    "        lambda boxes: tf.gather(boxes, tf.image.non_max_suppression(\n",
    "            boxes[:,:4],\n",
    "            boxes[:,4],\n",
    "            6,\n",
    "            score_threshold=0.5\n",
    "        )),\n",
    "        args,\n",
    "        infer_shape=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(output, target):\n",
    "    shape = output.shape\n",
    "    def make_mask(bboxes):\n",
    "        mask = tf.zeros([int(shape[1]), int(shape[2]), 6])\n",
    "        def add_data(bbox):\n",
    "            mask[int(bbox[0] * shape[2]), int(bbox[1] * shape[1]), 0] = 1\n",
    "            mask[int(bbox[0] * shape[2]), int(bbox[1] * shape[1]), 1:] = bbox\n",
    "            \n",
    "        tf.map_fn(add_data, bboxes)\n",
    "        return mask\n",
    "    obj_mask = tf.map_fn(make_mask, target)\n",
    "    \n",
    "    box_mask = tf.one_hot(tf.argmax(output[:, :, :, 4::25], axis=-1),\n",
    "                          depth=(shape[-1] / 25), axis=-1)\n",
    "    \n",
    "    return obj_mask, 1 - obj_mask[:, :, :, 0], box_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(output, mask):\n",
    "    x_min = tf.maximum(output[:,:,:,0::25] - output[:,:,:,2::25] / 2,\n",
    "                       mask[:,:,:,1] - mask[:,:,:,3] / 2)\n",
    "    x_max = tf.minimum(output[:,:,:,0::25] + output[:,:,:,2::25] / 2,\n",
    "                       mask[:,:,:,1] + mask[:,:,:,3] / 2)\n",
    "    y_min = tf.maximum(output[:,:,:,1::25] - output[:,:,:,3::25] / 2,\n",
    "                       mask[:,:,:,2] - mask[:,:,:,4] / 2)\n",
    "    y_max = tf.minimum(output[:,:,:,1::25] + output[:,:,:,3::25] / 2,\n",
    "                       mask[:,:,:,2] + mask[:,:,:,4] / 2)\n",
    "    \n",
    "    inter_area = tf.maximum(x_max - x_min, 0) * tf.maximum(y_max - y_min, 0)\n",
    "    area_1 = output[:,:,:,2::25] * output[:,:,:,3::25]\n",
    "    area_2 = mask[:,:,:,3] * mask[:,:,:,4]\n",
    "    return inter_area / (area_1 + area_2 - inter_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_loss(output, mask):\n",
    "    loss = 0\n",
    "    for i in range(int(output.shape[-1] / 25)):\n",
    "        loss += tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels=tf.one_hot(mask[:,:,:,-1]),\n",
    "            logits=output[:,:,:,(25 * i + 4):(25 * (i + 1))]\n",
    "        )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_loss(outputs, target):\n",
    "    loss = 0\n",
    "    for output in outputs:\n",
    "        obj_mask, noobj_mask, box_mask = get_masks(output, target)\n",
    "        loss += tf.reduce_mean(\n",
    "            obj_mask[:,:,:,0] * (box_mask * (L_COORD * (\n",
    "                tf.squared_difference(output[:,:,:,0::25], obj_mask[:,:,:,1]) +\n",
    "                tf.squared_difference(output[:,:,:,1::25], obj_mask[:,:,:,2]) +\n",
    "                tf.squared_difference(\n",
    "                    tf.sqrt(output[:,:,:,2::25]), tf.sqrt(obj_mask[:,:,:,3])\n",
    "                ) + tf.squared_difference(\n",
    "                    tf.sqrt(output[:,:,:,3::25]), tf.sqrt(obj_mask[:,:,:,4])\n",
    "                )\n",
    "            ) + tf.squared_difference(output[:,:,:,4::25], iou(output, obj_mask)))) +\n",
    "            classification_loss(output, obj_mask)\n",
    "        )\n",
    "        loss += L_NOOBJ * tf.reduce_mean(noobj_mask * tf.square(output[:,:,:,4::25]))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def darknet_model(features, labels, mode):  \n",
    "    features = tf.cast(features, dtype=tf.float32)\n",
    "    normalized = tf.map_fn(tf.image.per_image_standardization, features,\n",
    "                           infer_shape=False)\n",
    "    \n",
    "    # Feature extractor: Darknet53\n",
    "    x = conv_layer(inputs=normalized, filters=32, trainable=False)\n",
    "    x = darknet_block(x, 64, 1)\n",
    "    x = darknet_block(x, 128, 2)\n",
    "    l_36 = darknet_block(x, 256, 8)\n",
    "    l_61 = darknet_block(l_36, 512, 8)\n",
    "    x = darknet_block(l_61, 1024, 4, trainable=True)\n",
    "    \n",
    "    # YOLO model\n",
    "    x = conv_layer(x, filters=512, kernel_size=1)\n",
    "    x = conv_layer(x, filters=1024)\n",
    "    x = conv_layer(x, filters=512, kernel_size=1)\n",
    "    x = conv_layer(x, filters=1024)\n",
    "    l_79 = conv_layer(x, filters=512, kernel_size=1)\n",
    "    \n",
    "    x = conv_layer(l_79, filters=1024)\n",
    "    x = conv_layer(x, filters=75, kernel_size=1, activation=None, batch_normalize=False)\n",
    "    o_1 = yolo_layer(x, anchors=[(116, 90), (156, 198), (373, 326)])\n",
    "    \n",
    "    x = conv_layer(l_79, filters=256, kernel_size=1)\n",
    "    x = tf.image.resize_images(x, (int(x.shape[1]) * 2, int(x.shape[2]) * 2))\n",
    "    x = tf.concat([x, l_61], axis=-1)\n",
    "    x = conv_layer(x, filters=256, kernel_size=1)\n",
    "    x = conv_layer(x, filters=512)\n",
    "    x = conv_layer(x, filters=256, kernel_size=1)\n",
    "    x = conv_layer(x, filters=512)\n",
    "    l_91 = conv_layer(x, filters=256, kernel_size=1)\n",
    "    \n",
    "    x = conv_layer(x, filters=512)\n",
    "    x = conv_layer(x, filters=75, kernel_size=1, activation=None, batch_normalize=False)\n",
    "    o_2 = yolo_layer(x, anchors=[(30, 61), (62, 45), (59, 119)])\n",
    "    \n",
    "    x = conv_layer(l_91, filters=128, kernel_size=1)\n",
    "    x = tf.image.resize_images(x, (int(x.shape[1]) * 2, int(x.shape[2]) * 2))\n",
    "    x = tf.concat([x, l_36], axis=-1)\n",
    "    x = conv_layer(x, filters=128, kernel_size=1)\n",
    "    x = conv_layer(x, filters=256)\n",
    "    x = conv_layer(x, filters=128, kernel_size=1)\n",
    "    x = conv_layer(x, filters=256)\n",
    "    x = conv_layer(x, filters=128, kernel_size=1)\n",
    "    x = conv_layer(x, filters=256)\n",
    "    x = conv_layer(x, filters=75, kernel_size=1, activation=None, batch_normalize=False)\n",
    "    o_3 = yolo_layer(x, anchors=[(10, 13), (16, 30), (33, 23)])\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # Non-maximum suppression to remove overlapping boxes\n",
    "        output = non_max_suppr(o_1, o_2, o_3)\n",
    "        predictions = {\n",
    "            'images': tf.image.draw_bounding_boxes(features, output[:,:,:4]),\n",
    "            'labels': tf.argmax(output[:,:,5:], axis=-1)\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    \n",
    "    loss = yolo_loss([o_1, o_2, o_3], labels)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.contrib.estimator.TowerOptimizer(tf.train.AdamOptimizer(1e-4))\n",
    "        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('darknet_variables.pkl', 'rb') as vars_file:\n",
    "    warm_start = tf.estimator.WarmStartSettings(\n",
    "        ckpt_to_initialize_from='/tmp/tmpdark/',\n",
    "        vars_to_warm_start=pickle.load(vars_file)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.estimator.Estimator(\n",
    "    model_fn=tf.contrib.estimator.replicate_model_fn(darknet_model), model_dir='/tmp/tmpdarkyolo',\n",
    "    warm_start_from=warm_start, config=tf.estimator.RunConfig(\n",
    "        save_checkpoints_steps=150, save_summary_steps=10, log_step_count_steps=10\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_hook = tf.contrib.learn.monitors.replace_monitors_with_hooks(\n",
    "    [tf.contrib.learn.monitors.ValidationMonitor(\n",
    "        input_fn=lambda:train_input_fn()[1], every_n_steps=100, early_stopping_rounds=10\n",
    "    )],\n",
    "    model\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_steps = int(((1 - VALIDATION_SPLIT) * TRAIN_LENGTH * 8 / BATCH_SIZE) * EPOCHS)\n",
    "model.train(input_fn=lambda:train_input_fn()[0], hooks=[validation_hook],\n",
    "            max_steps=max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.evaluate(input_fn=test_input_fn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
