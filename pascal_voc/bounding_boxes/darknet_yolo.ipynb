{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from load_pvoc_data import load_data, TRAIN_LENGTH\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 4\n",
    "VALIDATION_SPLIT = 0.3\n",
    "L_COORD = 5\n",
    "L_NOOBJ = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    train_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda:load_data(\"train\"),\n",
    "        (tf.uint8, tf.int32),\n",
    "        (tf.TensorShape([None, None, 3]), tf.TensorShape([None, 5]))\n",
    "    )\n",
    "    train_dataset = train_dataset.shuffle(10000)\n",
    "    \n",
    "    val_length = int(VALIDATION_SPLIT * TRAIN_LENGTH * 8)\n",
    "    val_dataset = train_dataset.take(val_length).apply(\n",
    "        tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE))\n",
    "    train_dataset = train_dataset.skip(val_length).apply(\n",
    "        tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE)).repeat()\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_input_fn():\n",
    "    test_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda:load_data(\"test\"),\n",
    "        (tf.uint8, tf.int32),\n",
    "        (tf.TensorShape([None, None, 3]), tf.TensorShape([None]))\n",
    "    )\n",
    "    return test_dataset.apply(tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(inputs, filters=32, kernel_size=3, strides=1, activation=tf.nn.leaky_relu, batch_normalize=True,\n",
    "               trainable=True):\n",
    "    x = tf.layers.conv2d(inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides, padding='same',\n",
    "                         trainable=trainable)\n",
    "    if batch_normalize:\n",
    "        x = tf.layers.batch_normalization(x, trainable=trainable)\n",
    "    if activation is not None:\n",
    "        x = activation(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(inputs, filters, trainable=False):\n",
    "    x = conv_layer(inputs=inputs, filters=filters, kernel_size=1, trainable=trainable)\n",
    "    x = conv_layer(inputs=inputs, filters=(filters * 2), trainable=trainable)\n",
    "    return x + inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def darknet_block(inputs, filters, repetitions, trainable=False):\n",
    "    x = conv_layer(inputs=inputs, filters=filters, strides=2, trainable=trainable)\n",
    "    for i in range(repetitions):\n",
    "        x = residual_block(x, filters / 2, trainable=trainable)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_layer(inputs, anchors):\n",
    "    indices_w = tf.range(tf.shape(inputs)[2])\n",
    "    indices_h = tf.range(tf.shape(inputs)[1])\n",
    "    x_indices, y_indices = tf.meshgrid(indices_w, indices_h)\n",
    "    x_indices = tf.cast(x_indices, dtype=tf.float32)\n",
    "    y_indices = tf.cast(y_indices, dtype=tf.float32)\n",
    "    \n",
    "    stack = []\n",
    "    for i, anchor in enumerate(anchors):\n",
    "        stack.append((tf.sigmoid(inputs[:,:,:,25 * i + 0]) + x_indices) / tf.cast(tf.shape(inputs)[2], dtype=tf.float32))    # bx\n",
    "        stack.append((tf.sigmoid(inputs[:,:,:,25 * i + 0]) + y_indices) / tf.cast(tf.shape(inputs)[1], dtype=tf.float32))    # by\n",
    "        stack.append((tf.exp(inputs[:,:,:,25 * i + 2]) * anchor[0]) / tf.cast(tf.shape(inputs)[2], dtype=tf.float32))    # bw\n",
    "        stack.append((tf.exp(inputs[:,:,:,25 * i + 3]) * anchor[1]) / tf.cast(tf.shape(inputs)[1], dtype=tf.float32))    # bh\n",
    "        stack.append(tf.sigmoid(inputs[:,:,:,25 * i + 4]))\n",
    "        for j in range(5, 25):\n",
    "            stack.append(inputs[:,:,:,25 * i + j])\n",
    "        \n",
    "    return tf.stack(stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppr(*args):\n",
    "    for i, arg in enumerate(args):\n",
    "        for j in range(arg.shape[-1] / 25):\n",
    "            arg[:,:,:,25 * j + 0] = arg[:,:,:,25 * j + 1] - arg[:,:,:,25 * j + 3] / 2    # y_min\n",
    "            arg[:,:,:,25 * j + 1] = arg[:,:,:,25 * j + 0] - arg[:,:,:,25 * j + 2] / 2    # x_min\n",
    "            arg[:,:,:,25 * j + 2] = arg[:,:,:,25 * j + 1] + arg[:,:,:,25 * j + 3] / 2    # y_max\n",
    "            arg[:,:,:,25 * j + 3] = arg[:,:,:,25 * j + 0] + arg[:,:,:,25 * j + 2] / 2    # x_max\n",
    "        flattened = tf.reshape(\n",
    "            arg,\n",
    "            (-1, tf.shape(arg)[1] * tf.shape(arg)[2], tf.shape(arg)[3])\n",
    "        )\n",
    "        to_concat = []\n",
    "        for j in range(arg.shape[-1] / 25):\n",
    "            to_concat.append(flattened[:,:,(25 * j):(25 * (j + 1))])\n",
    "        args[i] = tf.concat(to_concat, axis=1)\n",
    "    args = tf.concat(args, axis=1)\n",
    "    return tf.map_fn(\n",
    "        lambda boxes: tf.gather(boxes, tf.image.non_max_suppression(\n",
    "            boxes[:,:4],\n",
    "            boxes[:,4],\n",
    "            6,\n",
    "            score_threshold=0.5\n",
    "        )),\n",
    "        args,\n",
    "        infer_shape=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(output, target):\n",
    "    shape = output.get_shape().as_list()\n",
    "    def make_mask(bboxes):\n",
    "        indices = tf.map_fn(lambda bbox: [\n",
    "            tf.cast(bbox[0] * shape[1], dtype=tf.int32),\n",
    "            tf.cast(bbox[1] * shape[0], dtype=tf.int32),\n",
    "        ], bboxes, dtype=[tf.int32] * 2)\n",
    "        updates = tf.map_fn(lambda bbox: [1.0, bbox[0], bbox[1], bbox[2], bbox[3],\n",
    "                                          bbox[4]], bboxes, dtype=[tf.float32] * 6)\n",
    "        return tf.scatter_nd(indices, updates, [shape[0], shape[1], 6])\n",
    "    obj_mask = tf.map_fn(make_mask, target, dtype=tf.float32)\n",
    "    \n",
    "    box_mask = tf.one_hot(tf.argmax(output[:, :, :, 4::25], axis=-1),\n",
    "                          depth=tf.cast(tf.shape(output)[-1] / 25, dtype=tf.int32),\n",
    "                          axis=-1)\n",
    "    \n",
    "    return obj_mask, 1 - obj_mask[:, :, :, 0], box_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(output, mask):\n",
    "    x_min = tf.maximum(output[:,:,:,0::25] - output[:,:,:,2::25] / 2,\n",
    "                       mask[:,:,:,1] - mask[:,:,:,3] / 2)\n",
    "    x_max = tf.minimum(output[:,:,:,0::25] + output[:,:,:,2::25] / 2,\n",
    "                       mask[:,:,:,1] + mask[:,:,:,3] / 2)\n",
    "    y_min = tf.maximum(output[:,:,:,1::25] - output[:,:,:,3::25] / 2,\n",
    "                       mask[:,:,:,2] - mask[:,:,:,4] / 2)\n",
    "    y_max = tf.minimum(output[:,:,:,1::25] + output[:,:,:,3::25] / 2,\n",
    "                       mask[:,:,:,2] + mask[:,:,:,4] / 2)\n",
    "    \n",
    "    inter_area = tf.maximum(x_max - x_min, 0) * tf.maximum(y_max - y_min, 0)\n",
    "    area_1 = output[:,:,:,2::25] * output[:,:,:,3::25]\n",
    "    area_2 = mask[:,:,:,3] * mask[:,:,:,4]\n",
    "    return inter_area / (area_1 + area_2 - inter_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_loss(output, mask):\n",
    "    loss = tf.map_fn(lambda i: tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels=tf.one_hot(tf.cast(mask[:,:,:,-1], dtype=tf.int32), depth=20),\n",
    "            logits=output[:,:,:,(25 * i + 4):(25 * (i + 1))]\n",
    "        ), tf.range(tf.cast(tf.shape(output)[-1] / 25, dtype=tf.int32)),\n",
    "                    dtype=tf.float32)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_loss(outputs, target):\n",
    "    loss = 0\n",
    "    for output in outputs:\n",
    "        obj_mask, noobj_mask, box_mask = get_masks(output, target)\n",
    "        loss += tf.reduce_mean(\n",
    "            obj_mask[:,:,:,0] * (box_mask * (L_COORD * (\n",
    "                tf.squared_difference(output[:,:,:,0::25], obj_mask[:,:,:,1]) +\n",
    "                tf.squared_difference(output[:,:,:,1::25], obj_mask[:,:,:,2]) +\n",
    "                tf.squared_difference(\n",
    "                    tf.sqrt(output[:,:,:,2::25]), tf.sqrt(obj_mask[:,:,:,3])\n",
    "                ) + tf.squared_difference(\n",
    "                    tf.sqrt(output[:,:,:,3::25]), tf.sqrt(obj_mask[:,:,:,4])\n",
    "                )\n",
    "            ) + tf.squared_difference(output[:,:,:,4::25], iou(output, obj_mask)))) +\n",
    "            classification_loss(output, obj_mask)\n",
    "        )\n",
    "        loss += L_NOOBJ * tf.reduce_mean(noobj_mask * tf.square(output[:,:,:,4::25]))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def darknet_model(features, labels, mode):  \n",
    "    features = tf.cast(features, dtype=tf.float32)\n",
    "    normalized = tf.map_fn(tf.image.per_image_standardization, features)\n",
    "    \n",
    "    # Feature extractor: Darknet53\n",
    "    x = conv_layer(inputs=normalized, filters=32, trainable=False)\n",
    "    x = darknet_block(x, 64, 1)\n",
    "    x = darknet_block(x, 128, 2)\n",
    "    l_36 = darknet_block(x, 256, 8)\n",
    "    l_61 = darknet_block(l_36, 512, 8)\n",
    "    x = darknet_block(l_61, 1024, 4, trainable=True)\n",
    "    \n",
    "    # YOLO model\n",
    "    x = conv_layer(x, filters=512, kernel_size=1)\n",
    "    x = conv_layer(x, filters=1024)\n",
    "    x = conv_layer(x, filters=512, kernel_size=1)\n",
    "    x = conv_layer(x, filters=1024)\n",
    "    l_79 = conv_layer(x, filters=512, kernel_size=1)\n",
    "    \n",
    "    x = conv_layer(l_79, filters=1024)\n",
    "    x = conv_layer(x, filters=75, kernel_size=1, activation=None, batch_normalize=False)\n",
    "    o_1 = yolo_layer(x, anchors=[(116, 90), (156, 198), (373, 326)])\n",
    "    \n",
    "    x = conv_layer(l_79, filters=256, kernel_size=1)\n",
    "    x = tf.image.resize_images(x, (tf.shape(x)[1] * 2, tf.shape(x)[2] * 2))\n",
    "    x = tf.concat([x, l_61], axis=-1)\n",
    "    x = conv_layer(x, filters=256, kernel_size=1)\n",
    "    x = conv_layer(x, filters=512)\n",
    "    x = conv_layer(x, filters=256, kernel_size=1)\n",
    "    x = conv_layer(x, filters=512)\n",
    "    l_91 = conv_layer(x, filters=256, kernel_size=1)\n",
    "    \n",
    "    x = conv_layer(x, filters=512)\n",
    "    x = conv_layer(x, filters=75, kernel_size=1, activation=None, batch_normalize=False)\n",
    "    o_2 = yolo_layer(x, anchors=[(30, 61), (62, 45), (59, 119)])\n",
    "    \n",
    "    x = conv_layer(l_91, filters=128, kernel_size=1)\n",
    "    x = tf.image.resize_images(x, (tf.shape(x)[1] * 2, tf.shape(x)[2] * 2))\n",
    "    x = tf.concat([x, l_36], axis=-1)\n",
    "    x = conv_layer(x, filters=128, kernel_size=1)\n",
    "    x = conv_layer(x, filters=256)\n",
    "    x = conv_layer(x, filters=128, kernel_size=1)\n",
    "    x = conv_layer(x, filters=256)\n",
    "    x = conv_layer(x, filters=128, kernel_size=1)\n",
    "    x = conv_layer(x, filters=256)\n",
    "    x = conv_layer(x, filters=75, kernel_size=1, activation=None, batch_normalize=False)\n",
    "    o_3 = yolo_layer(x, anchors=[(10, 13), (16, 30), (33, 23)])\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # Non-maximum suppression to remove overlapping boxes\n",
    "        output = non_max_suppr(o_1, o_2, o_3)\n",
    "        predictions = {\n",
    "            'images': tf.image.draw_bounding_boxes(features, output[:,:,:4]),\n",
    "            'labels': tf.argmax(output[:,:,5:], axis=-1)\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    \n",
    "    loss = yolo_loss([o_1, o_2, o_3], labels)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.contrib.estimator.TowerOptimizer(tf.train.AdamOptimizer(1e-4))\n",
    "        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('darknet_variables.pkl', 'rb') as vars_file:\n",
    "    warm_start = tf.estimator.WarmStartSettings(\n",
    "        ckpt_to_initialize_from='/tmp/tmpdark/',\n",
    "        vars_to_warm_start=pickle.load(vars_file)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Replicating the `model_fn` across ['/device:GPU:0'].  Variables are going to be placed on ['/device:GPU:0'].  Consolidation device is going to be /device:GPU:0.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpdarkyolo', '_tf_random_seed': None, '_save_summary_steps': 10, '_save_checkpoints_steps': 150, '_save_checkpoints_secs': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 10, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ffa6bfdeb70>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:Estimator's model_fn (<function _replicate_model_fn_with_mode.<locals>.single_device_model_fn at 0x7ffa49a69ae8>) includes params argument, but params are not passed to Estimator.\n"
     ]
    }
   ],
   "source": [
    "model = tf.estimator.Estimator(\n",
    "    model_fn=tf.contrib.estimator.replicate_model_fn(darknet_model),\n",
    "    model_dir='/tmp/tmpdarkyolo',\n",
    "    warm_start_from=warm_start, config=tf.estimator.RunConfig(\n",
    "        save_checkpoints_steps=150, save_summary_steps=10, log_step_count_steps=10\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_hook = tf.contrib.learn.monitors.replace_monitors_with_hooks(\n",
    "    [tf.contrib.learn.monitors.ValidationMonitor(\n",
    "        input_fn=lambda:train_input_fn()[1], every_n_steps=100, early_stopping_rounds=10\n",
    "    )],\n",
    "    model\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From <ipython-input-34-1260b3c44fea>:62: TowerOptimizer.__init__ (from tensorflow.contrib.estimator.python.estimator.replicate_model_fn) is deprecated and will be removed after 2018-05-31.\n",
      "Instructions for updating:\n",
      "Please use `tf.contrib.distribute.MirroredStrategy` instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/tmp/tmpdark/', vars_to_warm_start=['batch_normalization/beta', 'batch_normalization/gamma', 'batch_normalization/moving_mean', 'batch_normalization/moving_variance', 'batch_normalization_1/beta', 'batch_normalization_1/gamma', 'batch_normalization_1/moving_mean', 'batch_normalization_1/moving_variance', 'batch_normalization_10/beta', 'batch_normalization_10/gamma', 'batch_normalization_10/moving_mean', 'batch_normalization_10/moving_variance', 'batch_normalization_11/beta', 'batch_normalization_11/gamma', 'batch_normalization_11/moving_mean', 'batch_normalization_11/moving_variance', 'batch_normalization_12/beta', 'batch_normalization_12/gamma', 'batch_normalization_12/moving_mean', 'batch_normalization_12/moving_variance', 'batch_normalization_13/beta', 'batch_normalization_13/gamma', 'batch_normalization_13/moving_mean', 'batch_normalization_13/moving_variance', 'batch_normalization_14/beta', 'batch_normalization_14/gamma', 'batch_normalization_14/moving_mean', 'batch_normalization_14/moving_variance', 'batch_normalization_15/beta', 'batch_normalization_15/gamma', 'batch_normalization_15/moving_mean', 'batch_normalization_15/moving_variance', 'batch_normalization_16/beta', 'batch_normalization_16/gamma', 'batch_normalization_16/moving_mean', 'batch_normalization_16/moving_variance', 'batch_normalization_17/beta', 'batch_normalization_17/gamma', 'batch_normalization_17/moving_mean', 'batch_normalization_17/moving_variance', 'batch_normalization_18/beta', 'batch_normalization_18/gamma', 'batch_normalization_18/moving_mean', 'batch_normalization_18/moving_variance', 'batch_normalization_19/beta', 'batch_normalization_19/gamma', 'batch_normalization_19/moving_mean', 'batch_normalization_19/moving_variance', 'batch_normalization_2/beta', 'batch_normalization_2/gamma', 'batch_normalization_2/moving_mean', 'batch_normalization_2/moving_variance', 'batch_normalization_20/beta', 'batch_normalization_20/gamma', 'batch_normalization_20/moving_mean', 'batch_normalization_20/moving_variance', 'batch_normalization_21/beta', 'batch_normalization_21/gamma', 'batch_normalization_21/moving_mean', 'batch_normalization_21/moving_variance', 'batch_normalization_22/beta', 'batch_normalization_22/gamma', 'batch_normalization_22/moving_mean', 'batch_normalization_22/moving_variance', 'batch_normalization_23/beta', 'batch_normalization_23/gamma', 'batch_normalization_23/moving_mean', 'batch_normalization_23/moving_variance', 'batch_normalization_24/beta', 'batch_normalization_24/gamma', 'batch_normalization_24/moving_mean', 'batch_normalization_24/moving_variance', 'batch_normalization_25/beta', 'batch_normalization_25/gamma', 'batch_normalization_25/moving_mean', 'batch_normalization_25/moving_variance', 'batch_normalization_26/beta', 'batch_normalization_26/gamma', 'batch_normalization_26/moving_mean', 'batch_normalization_26/moving_variance', 'batch_normalization_27/beta', 'batch_normalization_27/gamma', 'batch_normalization_27/moving_mean', 'batch_normalization_27/moving_variance', 'batch_normalization_28/beta', 'batch_normalization_28/gamma', 'batch_normalization_28/moving_mean', 'batch_normalization_28/moving_variance', 'batch_normalization_29/beta', 'batch_normalization_29/gamma', 'batch_normalization_29/moving_mean', 'batch_normalization_29/moving_variance', 'batch_normalization_3/beta', 'batch_normalization_3/gamma', 'batch_normalization_3/moving_mean', 'batch_normalization_3/moving_variance', 'batch_normalization_30/beta', 'batch_normalization_30/gamma', 'batch_normalization_30/moving_mean', 'batch_normalization_30/moving_variance', 'batch_normalization_31/beta', 'batch_normalization_31/gamma', 'batch_normalization_31/moving_mean', 'batch_normalization_31/moving_variance', 'batch_normalization_32/beta', 'batch_normalization_32/gamma', 'batch_normalization_32/moving_mean', 'batch_normalization_32/moving_variance', 'batch_normalization_33/beta', 'batch_normalization_33/gamma', 'batch_normalization_33/moving_mean', 'batch_normalization_33/moving_variance', 'batch_normalization_34/beta', 'batch_normalization_34/gamma', 'batch_normalization_34/moving_mean', 'batch_normalization_34/moving_variance', 'batch_normalization_35/beta', 'batch_normalization_35/gamma', 'batch_normalization_35/moving_mean', 'batch_normalization_35/moving_variance', 'batch_normalization_36/beta', 'batch_normalization_36/gamma', 'batch_normalization_36/moving_mean', 'batch_normalization_36/moving_variance', 'batch_normalization_37/beta', 'batch_normalization_37/gamma', 'batch_normalization_37/moving_mean', 'batch_normalization_37/moving_variance', 'batch_normalization_38/beta', 'batch_normalization_38/gamma', 'batch_normalization_38/moving_mean', 'batch_normalization_38/moving_variance', 'batch_normalization_39/beta', 'batch_normalization_39/gamma', 'batch_normalization_39/moving_mean', 'batch_normalization_39/moving_variance', 'batch_normalization_4/beta', 'batch_normalization_4/gamma', 'batch_normalization_4/moving_mean', 'batch_normalization_4/moving_variance', 'batch_normalization_40/beta', 'batch_normalization_40/gamma', 'batch_normalization_40/moving_mean', 'batch_normalization_40/moving_variance', 'batch_normalization_41/beta', 'batch_normalization_41/gamma', 'batch_normalization_41/moving_mean', 'batch_normalization_41/moving_variance', 'batch_normalization_42/beta', 'batch_normalization_42/gamma', 'batch_normalization_42/moving_mean', 'batch_normalization_42/moving_variance', 'batch_normalization_43/beta', 'batch_normalization_43/gamma', 'batch_normalization_43/moving_mean', 'batch_normalization_43/moving_variance', 'batch_normalization_44/beta', 'batch_normalization_44/gamma', 'batch_normalization_44/moving_mean', 'batch_normalization_44/moving_variance', 'batch_normalization_45/beta', 'batch_normalization_45/gamma', 'batch_normalization_45/moving_mean', 'batch_normalization_45/moving_variance', 'batch_normalization_46/beta', 'batch_normalization_46/gamma', 'batch_normalization_46/moving_mean', 'batch_normalization_46/moving_variance', 'batch_normalization_47/beta', 'batch_normalization_47/gamma', 'batch_normalization_47/moving_mean', 'batch_normalization_47/moving_variance', 'batch_normalization_48/beta', 'batch_normalization_48/gamma', 'batch_normalization_48/moving_mean', 'batch_normalization_48/moving_variance', 'batch_normalization_49/beta', 'batch_normalization_49/gamma', 'batch_normalization_49/moving_mean', 'batch_normalization_49/moving_variance', 'batch_normalization_5/beta', 'batch_normalization_5/gamma', 'batch_normalization_5/moving_mean', 'batch_normalization_5/moving_variance', 'batch_normalization_50/beta', 'batch_normalization_50/gamma', 'batch_normalization_50/moving_mean', 'batch_normalization_50/moving_variance', 'batch_normalization_51/beta', 'batch_normalization_51/gamma', 'batch_normalization_51/moving_mean', 'batch_normalization_51/moving_variance', 'batch_normalization_6/beta', 'batch_normalization_6/gamma', 'batch_normalization_6/moving_mean', 'batch_normalization_6/moving_variance', 'batch_normalization_7/beta', 'batch_normalization_7/gamma', 'batch_normalization_7/moving_mean', 'batch_normalization_7/moving_variance', 'batch_normalization_8/beta', 'batch_normalization_8/gamma', 'batch_normalization_8/moving_mean', 'batch_normalization_8/moving_variance', 'batch_normalization_9/beta', 'batch_normalization_9/gamma', 'batch_normalization_9/moving_mean', 'batch_normalization_9/moving_variance', 'beta1_power', 'beta2_power', 'conv2d/bias', 'conv2d/kernel', 'conv2d_1/bias', 'conv2d_1/kernel', 'conv2d_10/bias', 'conv2d_10/kernel', 'conv2d_11/bias', 'conv2d_11/kernel', 'conv2d_12/bias', 'conv2d_12/kernel', 'conv2d_13/bias', 'conv2d_13/kernel', 'conv2d_14/bias', 'conv2d_14/kernel', 'conv2d_15/bias', 'conv2d_15/kernel', 'conv2d_16/bias', 'conv2d_16/kernel', 'conv2d_17/bias', 'conv2d_17/kernel', 'conv2d_18/bias', 'conv2d_18/kernel', 'conv2d_19/bias', 'conv2d_19/kernel', 'conv2d_2/bias', 'conv2d_2/kernel', 'conv2d_20/bias', 'conv2d_20/kernel', 'conv2d_21/bias', 'conv2d_21/kernel', 'conv2d_22/bias', 'conv2d_22/kernel', 'conv2d_23/bias', 'conv2d_23/kernel', 'conv2d_24/bias', 'conv2d_24/kernel', 'conv2d_25/bias', 'conv2d_25/kernel', 'conv2d_26/bias', 'conv2d_26/kernel', 'conv2d_27/bias', 'conv2d_27/kernel', 'conv2d_28/bias', 'conv2d_28/kernel', 'conv2d_29/bias', 'conv2d_29/kernel', 'conv2d_3/bias', 'conv2d_3/kernel', 'conv2d_30/bias', 'conv2d_30/kernel', 'conv2d_31/bias', 'conv2d_31/kernel', 'conv2d_32/bias', 'conv2d_32/kernel', 'conv2d_33/bias', 'conv2d_33/kernel', 'conv2d_34/bias', 'conv2d_34/kernel', 'conv2d_35/bias', 'conv2d_35/kernel', 'conv2d_36/bias', 'conv2d_36/kernel', 'conv2d_37/bias', 'conv2d_37/kernel', 'conv2d_38/bias', 'conv2d_38/kernel', 'conv2d_39/bias', 'conv2d_39/kernel', 'conv2d_4/bias', 'conv2d_4/kernel', 'conv2d_40/bias', 'conv2d_40/kernel', 'conv2d_41/bias', 'conv2d_41/kernel', 'conv2d_42/bias', 'conv2d_42/kernel', 'conv2d_43/bias', 'conv2d_43/kernel', 'conv2d_44/bias', 'conv2d_44/kernel', 'conv2d_45/bias', 'conv2d_45/kernel', 'conv2d_46/bias', 'conv2d_46/kernel', 'conv2d_47/bias', 'conv2d_47/kernel', 'conv2d_48/bias', 'conv2d_48/kernel', 'conv2d_49/bias', 'conv2d_49/kernel', 'conv2d_5/bias', 'conv2d_5/kernel', 'conv2d_50/bias', 'conv2d_50/kernel', 'conv2d_51/bias', 'conv2d_51/kernel', 'conv2d_6/bias', 'conv2d_6/kernel', 'conv2d_7/bias', 'conv2d_7/kernel', 'conv2d_8/bias', 'conv2d_8/kernel', 'conv2d_9/bias', 'conv2d_9/kernel', 'dense/bias', 'dense/kernel'], var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Warm-starting from: ('/tmp/tmpdark/',)\n",
      "INFO:tensorflow:Warm-starting variable: batch_normalization/beta; prev_var_name: Unchanged\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to get matching files on /tmp/tmpdark/: Not found: /tmp/tmpdark; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-5cbc4346ded2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmax_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mVALIDATION_SPLIT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mTRAIN_LENGTH\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m8\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m model.train(input_fn=lambda:train_input_fn()[0], hooks=[validation_hook],\n\u001b[0;32m----> 3\u001b[0;31m             max_steps=max_steps)\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1117\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1133\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1134\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m                                              saving_listeners)\n\u001b[0m\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1259\u001b[0m       logging.info('Warm-starting with WarmStartSettings: %s' %\n\u001b[1;32m   1260\u001b[0m                    (self._warm_start_settings,))\n\u001b[0;32m-> 1261\u001b[0;31m       \u001b[0mwarm_starting_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarm_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warm_start_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m     \u001b[0;31m# Check if the user created a loss summary, and add one if they didn't.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m     \u001b[0;31m# We assume here that the summary is called 'loss'. If it is not, we will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/training/warm_starting_util.py\u001b[0m in \u001b[0;36mwarm_start\u001b[0;34m(ckpt_to_initialize_from, vars_to_warm_start, var_name_to_vocab_info, var_name_to_prev_var_name)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m           \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0m_warm_start_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_to_initialize_from\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_var_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m   prev_var_name_not_used = set(\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/training/warm_starting_util.py\u001b[0m in \u001b[0;36m_warm_start_var\u001b[0;34m(var, prev_ckpt, prev_tensor_name)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;31m# Assume tensor name remains the same.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mprev_tensor_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_var_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m   \u001b[0mcheckpoint_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_ckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mprev_tensor_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/training/checkpoint_utils.py\u001b[0m in \u001b[0;36minit_from_checkpoint\u001b[0;34m(ckpt_dir_or_file, assignment_map)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \"\"\"\n\u001b[1;32m    182\u001b[0m   \u001b[0mckpt_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_checkpoint_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_dir_or_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m   \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_dir_or_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m   \u001b[0mvariable_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_to_shape_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m   for tensor_name_in_ckpt, current_var_or_name in sorted(\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/training/checkpoint_utils.py\u001b[0m in \u001b[0;36mload_checkpoint\u001b[0;34m(ckpt_dir_or_file)\u001b[0m\n\u001b[1;32m     61\u001b[0m     raise ValueError(\"Couldn't find 'checkpoint' file or checkpoints in \"\n\u001b[1;32m     62\u001b[0m                      \"given directory %s\" % ckpt_dir_or_file)\n\u001b[0;32m---> 63\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tf_api_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train.NewCheckpointReader'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to get matching files on /tmp/tmpdark/: Not found: /tmp/tmpdark; No such file or directory"
     ]
    }
   ],
   "source": [
    "max_steps = int(((1 - VALIDATION_SPLIT) * TRAIN_LENGTH * 8 / BATCH_SIZE) * EPOCHS)\n",
    "model.train(input_fn=lambda:train_input_fn()[0], hooks=[validation_hook],\n",
    "            max_steps=max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.evaluate(input_fn=test_input_fn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
