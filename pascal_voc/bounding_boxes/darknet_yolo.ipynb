{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from load_pvoc_data import load_data, TRAIN_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 4\n",
    "VALIDATION_SPLIT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(img, lbl):\n",
    "    crop_img = tf.image.central_crop(img, 1)\n",
    "    resized = tf.image.resize_images(img, (256, 256))\n",
    "    norm_img = tf.image.per_image_standardization(resized)\n",
    "    \n",
    "    one_hot = tf.one_hot(lbl, 20)\n",
    "    #summed = tf.reduce_sum(one_hot, axis=-2)\n",
    "    #multi_hot = tf.where(\n",
    "    #    tf.equal(summed, 0), tf.zeros_like(summed, dtype=tf.float32), tf.ones_like(summed, dtype=tf.float32)\n",
    "    #)\n",
    "    return norm_img, one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    train_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda:load_data(\"train\"),\n",
    "        (tf.uint8, tf.int32),\n",
    "        (tf.TensorShape([None, None, 3]), tf.TensorShape([None]))\n",
    "    )\n",
    "    train_dataset = train_dataset.map(preprocessing).shuffle(10000).apply(tf.contrib.data.assert_element_shape((\n",
    "        [256, 256, 3],\n",
    "        [20]\n",
    "    )))\n",
    "    \n",
    "    val_length = int(VALIDATION_SPLIT * TRAIN_LENGTH * 8)\n",
    "    val_dataset = train_dataset.take(val_length).apply(\n",
    "        tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE))\n",
    "    train_dataset = train_dataset.skip(val_length).apply(\n",
    "        tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE)).repeat()\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_input_fn():\n",
    "    test_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda:load_data(\"test\"),\n",
    "        (tf.uint8, tf.int32),\n",
    "        (tf.TensorShape([None, None, 3]), tf.TensorShape([None]))\n",
    "    )\n",
    "    test_dataset = test_dataset.map(preprocessing).apply(tf.contrib.data.assert_element_shape((\n",
    "        [256, 256, 3],\n",
    "        [20]\n",
    "    )))\n",
    "    return test_dataset.apply(tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(inputs, filters=32, kernel_size=3, strides=1, activation=tf.nn.leaky_relu, batch_normalize=True\n",
    "               trainable=True):\n",
    "    x = tf.layers.conv2d(inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides, padding='same',\n",
    "                         trainable=trainable)\n",
    "    if batch_normalize:\n",
    "        x = tf.layers.batch_normalization(x, trainable=trainable)\n",
    "    if activation is not None:\n",
    "        x = activation(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(inputs, filters, trainable=False):\n",
    "    x = conv_layer(inputs=inputs, filters=filters, kernel_size=1, trainable=trainable)\n",
    "    x = conv_layer(inputs=inputs, filters=(filters * 2), trainable=trainable)\n",
    "    return x + inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def darknet_block(inputs, filters, repetitions, trainable=False):\n",
    "    x = conv_layer(inputs=inputs, filters=filters, strides=2, trainable=trainable)\n",
    "    for i in range(repetitions):\n",
    "        x = residual_block(x, filters / 2, trainable=trainable)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_layer(inputs, anchors):\n",
    "    indices_w = tf.range(int(inputs.shape[2]))\n",
    "    indices_h = tf.range(int(inputs.shape[1]))\n",
    "    x_indices, y_indices = tf.meshgrid(indices_w, indices_h)\n",
    "    \n",
    "    for i, anchor in enumerate(anchors):\n",
    "         b_x = tf.sigmoid(inputs[:,:,:,25 * i + 0]) + x_indices\n",
    "         b_y = tf.sigmoid(inputs[:,:,:,25 * i + 0]) + y_indices\n",
    "         b_w = tf.exp(inputs[:,:,:,25 * i + 2]) * anchor[0]\n",
    "         b_h = tf.exp(inputs[:,:,:,25 * i + 3]) * anchor[1]\n",
    "        \n",
    "        inputs[:,:,:,25 * i + 0] = (b_y - b_h / 2) / int(inputs.shape[1])   # y_min\n",
    "        inputs[:,:,:,25 * i + 1] = (b_x - b_w / 2) / int(inputs.shape[2])   # x_min\n",
    "        inputs[:,:,:,25 * i + 2] = (b_y + b_h / 2) / int(inputs.shape[1])   # y_max\n",
    "        inputs[:,:,:,25 * i + 3] = (b_x + b_w / 2) / int(inputs.shape[2])   # x_max\n",
    "        \n",
    "        inputs[:,:,:,25 * i + 4] = tf.sigmoid(inputs[:,:,:,25 * i + 4])\n",
    "        \n",
    "    flattened = tf.reshape(inputs, (-1, int(inputs.shape[1] * inputs.shape[2]), int(inputs.shape[3])))\n",
    "    return tf.concat([flattened[:,:,:25], flattened[:,:,25:50], flattened[:,:,50:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def darknet_model(features, labels, mode):  \n",
    "    features = tf.cast(features, dtype=tf.float32)\n",
    "    \n",
    "    # Feature extractor: Darknet53\n",
    "    x = conv_layer(inputs=features, filters=32, trainable=False)\n",
    "    x = darknet_block(x, 64, 1)\n",
    "    x = darknet_block(x, 128, 2)\n",
    "    l_36 = darknet_block(x, 256, 8)\n",
    "    l_61 = darknet_block(l_36, 512, 8)\n",
    "    x = darknet_block(l_61, 1024, 4, trainable=True)\n",
    "    \n",
    "    # YOLO model\n",
    "    x = conv_layer(x, filters=512, kernel_size=1)\n",
    "    x = conv_layer(x, filters=1024)\n",
    "    x = conv_layer(x, filters=512, kernel_size=1)\n",
    "    x = conv_layer(x, filters=1024)\n",
    "    l_79 = conv_layer(x, filters=512, kernel_size=1)\n",
    "    \n",
    "    x = conv_layer(l_79, filters=1024)\n",
    "    x = conv_layer(x, filters=75, kernel_size=1, activation=None, batch_normalize=False)\n",
    "    o_1 = yolo_layer(x, anchors=[(116, 90), (156, 198), (373, 326)])\n",
    "    \n",
    "    x = conv_layer(l_79, filters=256, kernel_size=1)\n",
    "    x = tf.image.resize_images(x, (int(x.shape[1]) * 2, int(x.shape[2]) * 2))\n",
    "    x = tf.concat([x, l_61], axis=-1)\n",
    "    x = conv_layer(x, filters=256, kernel_size=1)\n",
    "    x = conv_layer(x, filters=512)\n",
    "    x = conv_layer(x, filters=256, kernel_size=1)\n",
    "    x = conv_layer(x, filters=512)\n",
    "    l_91 = conv_layer(x, filters=256, kernel_size=1)\n",
    "    \n",
    "    x = conv_layer(x, filters=512)\n",
    "    x = conv_layer(x, filters=75, kernel_size=1, activation=None, batch_normalize=False)\n",
    "    o_2 = yolo_layer(x, anchors=[(30, 61), (62, 45), (59, 119)])\n",
    "    \n",
    "    x = conv_layer(l_91, filters=128, kernel_size=1)\n",
    "    x = tf.image.resize_images(x, (int(x.shape[1]) * 2, int(x.shape[2]) * 2))\n",
    "    x = tf.concat([x, l_36], axis=-1)\n",
    "    x = conv_layer(x, filters=128, kernel_size=1)\n",
    "    x = conv_layer(x, filters=256)\n",
    "    x = conv_layer(x, filters=128, kernel_size=1)\n",
    "    x = conv_layer(x, filters=256)\n",
    "    x = conv_layer(x, filters=128, kernel_size=1)\n",
    "    x = conv_layer(x, filters=256)\n",
    "    x = conv_layer(x, filters=75, kernel_size=1, activation=None, batch_normalize=False)\n",
    "    o_3 = yolo_layer(x, anchors=[(10, 13), (16, 30), (33, 23)])\n",
    "    \n",
    "    output = tf.concat([o_1, o_2, o_3])\n",
    "    bboxes = tf.map_fn(\n",
    "        lambda boxes: tf.gather(boxes, tf.image.non_max_suppression(boxes[:,:4], boxes[:,4], 6, score_threshold=0.5)),\n",
    "        output,\n",
    "        infer_shape=False\n",
    "    )\n",
    "    output = bboxes\n",
    "    output[:,:,0] = (bboxes[:,:,1] + bboxes[:,:,3]) / 2   # x-center\n",
    "    output[:,:,1] = (bboxes[:,:,0] + bboxes[:,:,2]) / 2   # y-center\n",
    "    output[:,:,2] = (bboxes[:,:,3] - bboxes[:,:,1]) / 2   # width\n",
    "    output[:,:,3] = (bboxes[:,:,2] - bboxes[:,:,0]) / 2   # height\n",
    "    \n",
    "    # TODO\n",
    "    \"\"\"\n",
    "    classes = tf.where(tf.sigmoid(x) >= 0.5, tf.ones_like(x, dtype=tf.float32), tf.zeros_like(x, dtype=tf.float32))\n",
    "    correct_prediction = tf.equal(classes, labels)\n",
    "    acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    acc = tf.identity(acc, name='accuracy_tensor')\n",
    "    \n",
    "    predictions = {'classes': classes, 'accuracy': acc}\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    \n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=x)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    tf.summary.scalar('accuracy', acc)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    \"\"\"\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.contrib.estimator.TowerOptimizer(tf.train.AdamOptimizer(1e-4))\n",
    "        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    eval_metric_ops = {'accuracy': tf.metrics.accuracy(labels=labels, predictions=classes)}\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss,\n",
    "                                      eval_metric_ops=eval_metric_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors_to_log = {'accuracy': 'accuracy_tensor'}\n",
    "logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_warm = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_start = tf.estimator.WarmStartSettings(ckpt_to_initialize_from='/tmp/tmpdark/', vars_to_warm_start=vars_warm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.estimator.Estimator(\n",
    "    model_fn=tf.contrib.estimator.replicate_model_fn(darknet_model), model_dir='/tmp/tmpdarkyolo',\n",
    "    warm_start_from=warm_start, config=tf.estimator.RunConfig(\n",
    "        save_checkpoints_steps=150, save_summary_steps=10, log_step_count_steps=10\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_hook = tf.contrib.learn.monitors.replace_monitors_with_hooks(\n",
    "    [tf.contrib.learn.monitors.ValidationMonitor(\n",
    "        input_fn=lambda:train_input_fn()[1], every_n_steps=100, early_stopping_rounds=10\n",
    "    )],\n",
    "    model\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_steps = int(((1 - VALIDATION_SPLIT) * TRAIN_LENGTH * 8 / BATCH_SIZE) * EPOCHS)\n",
    "model.train(input_fn=lambda:train_input_fn()[0], hooks=[logging_hook, validation_hook],\n",
    "            max_steps=max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.evaluate(input_fn=test_input_fn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
